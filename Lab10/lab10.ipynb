{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib as hl\n",
    "import random as rd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_win(state):\n",
    "    state=np.array(state).reshape(3,3)\n",
    "     # Controllare le righe e le colonne\n",
    "    for i in range(3):\n",
    "        # Controllare le righe\n",
    "        if all(state[i, j] == state[i, 0] and state[i, 0] != ' ' for j in range(3)):\n",
    "            return True\n",
    "        # Controllare le colonne\n",
    "        if all(state[j, i] == state[0, i] and state[0, i] != ' ' for j in range(3)):\n",
    "            return True\n",
    "    # Controllare le diagonali\n",
    "    if state[0, 0] == state[1, 1] == state[2, 2] and state[0, 0] != ' ':\n",
    "        return True\n",
    "    if state[0, 2] == state[1, 1] == state[2, 0] and state[0, 2] != ' ':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, symbol):\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def make_move(self, state):\n",
    "        empty_cells = np.where(np.array(state) == ' ')[0]\n",
    "        # print(\"random player empty cells\",empty_cells)\n",
    "        action = np.random.choice(len(empty_cells))\n",
    "        # print(\"random player action\",action)    \n",
    "        state[action] = self.symbol\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearningAgent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state):\n",
    "    if state.count(' ') == 0:\n",
    "        return 0.5 #draw\n",
    "    elif check_win(state)==True :\n",
    "        return 10 #q-learning agent won\n",
    "    else:\n",
    "        # check if random player won\n",
    "        shadow_state = state.copy()\n",
    "        shadow_state = RandomPlayer('O').make_move(shadow_state)\n",
    "        if check_win(shadow_state)==True:\n",
    "            return -10 #q-learning agent lost\n",
    "        else:\n",
    "            return 0.1 #reward for generic move, game not ended  \n",
    "        \n",
    "\n",
    "\n",
    "def get_qvalue_max(qtable,state,action):\n",
    "    best_act = None\n",
    "    val_max = float('-inf')\n",
    "    \n",
    "    if action is not None and (tuple(state),action) not in qtable:\n",
    "        return action,0\n",
    "    else:\n",
    "        for key, val in qtable.items():\n",
    "            if key[0] == tuple(state) and val > val_max:\n",
    "                val_max = val\n",
    "                best_act = key[1]\n",
    "\n",
    "    return best_act,val_max\n",
    "\n",
    "def print_board(state):\n",
    "    state_as_list = [list(state[i:i+3]) for i in range(0, len(state), 3)]\n",
    "    for row in state_as_list:\n",
    "        print(row)\n",
    "    print(\"-----------------\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QAGENTMODE=\"RANDOM\"\n",
    "\n",
    "class QLearningAgent():\n",
    "    def __init__(self, symbol):\n",
    "        self.symbol = symbol\n",
    "        self.eps=0.4\n",
    "    \n",
    "    def make_move(self, state):\n",
    "        # print(\"qply_state\",state)\n",
    "        possible_moves=np.where(np.array(state)==' ')[0]\n",
    "        # print(\"board\",state)\n",
    "        # print(\"qply possible_moves\",possible_moves)\n",
    "        # action=np.random.choice(possible_moves)\n",
    "        \n",
    "        if QAGENTMODE==\"EPSILON_GREEDY\":\n",
    "            # epsilon greedy\n",
    "            if rd.random() < self.eps:\n",
    "                action=np.random.choice(possible_moves)\n",
    "                # print(\"qplayer action explorative\",action)\n",
    "            else:\n",
    "                if all((tuple(state),val) not in qtable for val in possible_moves)==True:\n",
    "                    action=np.random.choice(possible_moves)\n",
    "                    # print(\"qplayer action explorative\",action)\n",
    "                else:\n",
    "                    action,_=get_qvalue_max(qtable,state,None)\n",
    "                    # print(\"qplayer action exploitative\",action)\n",
    "\n",
    "        elif QAGENTMODE==\"RANDOM\":\n",
    "            action=np.random.choice(possible_moves)\n",
    "        else:\n",
    "            print(\"ERROR MODE NOT FOUND\")\n",
    "            return None\n",
    "            \n",
    "        hashable_state=tuple(state)\n",
    "\n",
    "        # print(hashable_state,action)\n",
    "\n",
    "        if (hashable_state,action) not in qtable:\n",
    "            #if new state,action tuple is discovered assign as q-value \n",
    "            # a random number between -1 and 1\n",
    "            qtable[(hashable_state,action)]=np.random.uniform(-1,1)\n",
    "            # print(\"new move\")\n",
    "            # print(\"qtable\",qtable)\n",
    "            # print(\"action:\",action)\n",
    "            state[action]=self.symbol #update the state\n",
    "            return state \n",
    "        else:\n",
    "            #if a value already exists for the state, update the q-value\n",
    "            state[action]=self.symbol #update the state \n",
    "            _,qvalue_max=get_qvalue_max(qtable,tuple(state),action)\n",
    "            # print(\"already discovered\")\n",
    "            # print(\"qvalue_max\",qvalue_max)\n",
    "            # print(\"qtable\",qtable)\n",
    "            curr_value=qtable[(hashable_state,action)]\n",
    "            qtable[(hashable_state,action)]=(1-lr)*curr_value+lr*(reward(state)+discount*qvalue_max)\n",
    "            # print(\"updated qtable\",qtable)\n",
    "        return state\n",
    "\n",
    "    def use_only_qtable(self,state):\n",
    "        action,_=get_qvalue_max(qtable,state,None)\n",
    "        if action is None: #new tuple state,action not found in training\n",
    "            action=np.random.choice(np.where(np.array(state)==' ')[0])\n",
    "        state[action]=self.symbol\n",
    "        return state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5741ba65a5a349b9bcb6184a0abd9492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284, 865, 851]\n",
      "Q-learning agent win rate: 42.55 %\n",
      "Random player win rate: 43.25 %\n",
      "Draw rate: 14.2 %\n",
      "qtable size: 4906\n"
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "discount=0.5\n",
    "\n",
    "qtable=dict() #key:(state,action) tuple, value: q-value\n",
    "\n",
    "rndply= RandomPlayer('X')\n",
    "qply=QLearningAgent('O')\n",
    "results=[0,0,0] #draws,player1 wins,player2 wins\n",
    "games=2000\n",
    "\n",
    "for i in tqdm(range(games)):\n",
    "    board=[' ',' ',' ',' ',' ',' ',' ',' ',' ']\n",
    "    player=1 if games<games//2 else 0\n",
    "    # print(\"game:{}\",i+1)\n",
    "    while check_win(board)==False and board.count(' ')>0:\n",
    "        if player==1:\n",
    "            # completly random Q-learning agent\n",
    "            board=qply.make_move(board)\n",
    "        else:\n",
    "            board=rndply.make_move(board)\n",
    "        player=1-player\n",
    "        # print_board(board)\n",
    "        # print(\"spaces left\",board.count(' '))\n",
    "    if board.count(' ')==0:\n",
    "        # print(\"Game over,draw!\")\n",
    "        results[0]+=1\n",
    "    else:\n",
    "        # print(\"Game over,won by player\",player+1,\"!\")\n",
    "        results[player+1]+=1\n",
    "print(results)\n",
    "print(\"Q-learning agent win rate:\",results[2]/games*100,\"%\")\n",
    "print(\"Random player win rate:\",results[1]/games*100,\"%\")\n",
    "print(\"Draw rate:\",results[0]/games*100,\"%\")\n",
    "print(\"qtable size:\",len(qtable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play against human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer():\n",
    "    def __init__(self, symbol):\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def make_move(self, state):\n",
    "        while True:\n",
    "            action = int(input(\"Choose your move, avaible moves are: \" + str(np.where(np.array(state) == ' ')[0]) + \"\\n\"))\n",
    "            if state[action] == ' ':\n",
    "                state[action] = self.symbol\n",
    "                return state\n",
    "            else:\n",
    "                print(\"Invalid move!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ']\n",
      "[' ', ' ', ' ']\n",
      "['O', ' ', ' ']\n",
      "-----------------\n",
      "['X', ' ', ' ']\n",
      "[' ', ' ', ' ']\n",
      "['O', ' ', ' ']\n",
      "-----------------\n",
      "['X', ' ', ' ']\n",
      "[' ', ' ', ' ']\n",
      "['O', ' ', 'O']\n",
      "-----------------\n",
      "['X', ' ', 'X']\n",
      "[' ', ' ', ' ']\n",
      "['O', ' ', 'O']\n",
      "-----------------\n",
      "['X', 'O', 'X']\n",
      "[' ', ' ', ' ']\n",
      "['O', ' ', 'O']\n",
      "-----------------\n",
      "['X', 'O', 'X']\n",
      "[' ', ' ', ' ']\n",
      "['O', 'X', 'O']\n",
      "-----------------\n",
      "['X', 'O', 'X']\n",
      "[' ', 'O', ' ']\n",
      "['O', 'X', 'O']\n",
      "-----------------\n",
      "['X', 'O', 'X']\n",
      "[' ', 'O', 'X']\n",
      "['O', 'X', 'O']\n",
      "-----------------\n",
      "['X', 'O', 'X']\n",
      "['O', 'O', 'X']\n",
      "['O', 'X', 'O']\n",
      "-----------------\n",
      "Draw!\n"
     ]
    }
   ],
   "source": [
    "qply=QLearningAgent('O')\n",
    "hlpy= HumanPlayer('X')\n",
    "\n",
    "board=[' ',' ',' ',' ',' ',' ',' ',' ',' ']\n",
    "while check_win(board)==False and board.count(' ')>0:\n",
    "    if player==1:\n",
    "        # Q-learning 4agent\n",
    "        board=qply.use_only_qtable(board)\n",
    "    else:\n",
    "        board=hlpy.make_move(board)\n",
    "    player=1-player\n",
    "    print_board(board)\n",
    "if board.count(' ')==0:\n",
    "    print(\"Draw!\")\n",
    "elif player==1:\n",
    "    print(\"Q-learning agent won!\")\n",
    "else:\n",
    "    print(\"Human player won!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
